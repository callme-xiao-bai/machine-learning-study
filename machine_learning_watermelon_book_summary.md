# 《机器学习》（西瓜书）知识点总结

## 目录
1. [第1章 绪论](#第1章-绪论)
   - [1.1 基本概念](#11-基本概念)
   - [1.2 基本术语](#12-基本术语)
   - [1.3 发展历程](#13-发展历程)
   - [1.4 本章小结](#14-本章小结)
   - [1.5 习题与思考](#15-习题与思考)

2. [第2章 模型评估与选择](#第2章-模型评估与选择)
   - [2.1 经验误差与过拟合](#21-经验误差与过拟合)
   - [2.2 性能度量](#22-性能度量)
   - [2.3 比较检验](#23-比较检验)
   - [2.4 本章小结](#24-本章小结)
   - [2.5 习题与思考](#25-习题与思考)

3. [第3章 线性模型](#第3章-线性模型)
   - [3.1 基本形式](#31-基本形式)
   - [3.2 对数几率回归](#32-对数几率回归)
   - [3.3 线性判别分析](#33-线性判别分析)
   - [3.4 多分类学习](#34-多分类学习)
   - [3.5 类别不平衡问题](#35-类别不平衡问题)
   - [3.6 本章小结](#36-本章小结)
   - [3.7 习题与思考](#37-习题与思考)

4. [第4章 决策树](#第4章-决策树)
   - [4.1 基本流程](#41-基本流程)
   - [4.2 特征选择](#42-特征选择)
   - [4.3 剪枝处理](#43-剪枝处理)
   - [4.4 连续值与缺失值](#44-连续值与缺失值)
   - [4.5 多变量决策树](#45-多变量决策树)
   - [4.6 本章小结](#46-本章小结)
   - [4.7 习题与思考](#47-习题与思考)

5. [第5章 神经网络](#第5章-神经网络)
   - [5.1 神经元模型](#51-神经元模型)
   - [5.2 感知机与多层网络](#52-感知机与多层网络)
   - [5.3 误差逆传播算法](#53-误差逆传播算法)
   - [5.4 全局最小与局部极小](#54-全局最小与局部极小)
   - [5.5 其他常见神经网络](#55-其他常见神经网络)
   - [5.6 本章小结](#56-本章小结)
   - [5.7 习题与思考](#57-习题与思考)

6. [第6章 支持向量机](#第6章-支持向量机)
   - [6.1 间隔与支持向量](#61-间隔与支持向量)
   - [6.2 对偶问题](#62-对偶问题)
   - [6.3 核函数](#63-核函数)
   - [6.4 软间隔与正则化](#64-软间隔与正则化)
   - [6.5 支持向量回归](#65-支持向量回归)
   - [6.6 本章小结](#66-本章小结)
   - [6.7 习题与思考](#67-习题与思考)

7. [第7章 贝叶斯分类器](#第7章-贝叶斯分类器)
   - [7.1 贝叶斯决策论](#71-贝叶斯决策论)
   - [7.2 极大似然估计](#72-极大似然估计)
   - [7.3 朴素贝叶斯分类器](#73-朴素贝叶斯分类器)
   - [7.4 半朴素贝叶斯分类器](#74-半朴素贝叶斯分类器)
   - [7.5 贝叶斯网络](#75-贝叶斯网络)
   - [7.6 本章小结](#76-本章小结)
   - [7.7 习题与思考](#77-习题与思考)

8. [第8章 集成学习](#第8章-集成学习)
   - [8.1 个体与集成](#81-个体与集成)
   - [8.2 Boosting](#82-boosting)
   - [8.3 Bagging与随机森林](#83-bagging与随机森林)
   - [8.4 结合策略](#84-结合策略)
   - [8.5 多样性](#85-多样性)
   - [8.6 本章小结](#86-本章小结)
   - [8.7 习题与思考](#87-习题与思考)

9. [第9章 聚类](#第9章-聚类)
   - [9.1 聚类任务](#91-聚类任务)
   - [9.2 距离计算](#92-距离计算)
   - [9.3 原型聚类](#93-原型聚类)
   - [9.4 密度聚类](#94-密度聚类)
   - [9.5 层次聚类](#95-层次聚类)
   - [9.6 本章小结](#96-本章小结)
   - [9.7 习题与思考](#97-习题与思考)

10. [第10章 降维与度量学习](#第10章-降维与度量学习)
    - [10.1 k近邻学习](#101-k近邻学习)
    - [10.2 降维](#102-降维)
    - [10.3 度量学习](#103-度量学习)
    - [10.4 本章小结](#104-本章小结)
    - [10.5 习题与思考](#105-习题与思考)

---

## 第1章 绪论
### 1.1 基本概念
#### 机器学习的定义
- Mitchell给出的机器学习定义：对于某类任务T和性能度量P，如果一个计算机程序的性能P随着经验E而自动改进，则称这个计算机程序从经验E中学习。
- 机器学习的本质：从数据中自动分析获得模型，并利用模型对未知数据进行预测。

#### 机器学习的分类
1. **监督学习（Supervised Learning）**
   - 定义：从标注数据中学习预测模型
   - 典型任务：分类、回归
   - 特点：训练数据包含输入和对应的标签/输出

2. **无监督学习（Unsupervised Learning）**
   - 定义：从无标注数据中发现隐含的结构
   - 典型任务：聚类、降维、密度估计
   - 特点：训练数据只有输入，没有标签

3. **半监督学习（Semi-supervised Learning）**
   - 定义：同时利用有标注和无标注数据学习
   - 应用场景：标注数据获取成本高
   - 特点：结合监督学习和无监督学习的优势

4. **强化学习（Reinforcement Learning）**
   - 定义：通过与环境交互学习最优决策
   - 核心概念：状态、动作、奖励
   - 特点：通过试错学习，延迟反馈

#### 泛化能力
- 定义：模型对未见示例的预测能力
- 影响因素：
  1. 训练数据的质量和数量
  2. 模型的复杂度
  3. 归纳偏好的合理性
- 评估方法：
  1. 留出法
  2. 交叉验证
  3. 自助法

### 1.2 基本术语
#### 数据集相关概念
1. **数据集（Data Set）**
   - 训练集（Training Set）：用于训练模型
   - 验证集（Validation Set）：用于模型选择和参数调优
   - 测试集（Test Set）：用于评估模型泛化性能

2. **特征（Feature）**
   - 属性（Attribute）：数据集中的列
   - 属性值（Attribute Value）：属性的取值
   - 属性空间（Attribute Space）：属性值的范围

3. **样本（Sample）**
   - 示例（Instance）：数据集中的行
   - 特征向量（Feature Vector）：样本的数值表示
   - 维数（Dimensionality）：特征的数量

#### 学习相关概念
1. **假设（Hypothesis）**
   - 定义：学习算法在学习过程中对真实规律的猜测
   - 假设空间（Hypothesis Space）：所有可能假设的集合
   - 版本空间（Version Space）：与训练集一致的假设集合

2. **归纳偏好（Inductive Bias）**
   - 定义：学习算法在学习过程中对某种类型假设的偏好
   - 必要性：没有免费的午餐定理（NFL定理）
   - 常见偏好：
     - 奥卡姆剃刀原则：简单性偏好
     - 最大间隔原则：间隔最大化偏好

### 1.3 发展历程
#### 机器学习的主要范式
1. **符号主义学习**
   - 时期：20世纪50-60年代
   - 特点：基于逻辑推理和知识表示
   - 代表：决策树、规则学习

2. **连接主义学习**
   - 时期：20世纪80年代至今
   - 特点：基于神经网络的仿生学习
   - 代表：深度学习、卷积神经网络

3. **统计学习**
   - 时期：20世纪90年代至今
   - 特点：基于统计理论的机器学习方法
   - 代表：SVM、贝叶斯方法

#### 重要里程碑
1. 1943年：McCulloch-Pitts神经元模型
2. 1957年：感知机（Perceptron）
3. 1986年：反向传播算法（BP）
4. 1995年：支持向量机（SVM）
5. 2006年：深度学习兴起
6. 2012年：深度学习在图像识别中的突破
7. 2017年：Transformer架构提出

### 本章小结
1. 机器学习是人工智能的核心，其目标是让计算机系统能够自动改进其性能
2. 机器学习任务可分为监督学习、无监督学习、半监督学习和强化学习
3. 泛化能力是机器学习算法的核心评价标准
4. 没有免费的午餐定理说明了归纳偏好的重要性
5. 机器学习经历了符号主义、连接主义和统计学习三个主要阶段

### 习题与思考
1. 为什么说机器学习算法都有归纳偏好？
    1.数据有限 2.处理多个不确定性 3 避免过拟合
2. 监督学习、无监督学习、半监督学习和强化学习各自适用于什么场景？
    1.监督学习：有标注数据 有明确的答案 2.无监督学习：无标注数据 没有标准答案 3.半监督学习：有标注数据和无标注数据 适合用于无法确定标准数据的情况 4.强化学习：与环境交互 倾向延迟反馈
3. 如何理解机器学习中的"没有免费的午餐"定理？
    没有通用的算法,因地制宜,everything takes a price
4. 为什么需要将数据集分成训练集、验证集和测试集？
    1.训练集用于训练模型 2.验证集用于模型选择和参数调优 3.测试集用于评估模型泛化性能
5. 机器学习中的"维数灾难"是什么？如何缓解？
    1.维数灾难是数据量过大,变量过多,导致模型过于复杂 2.缓解方法是增加训练数据/增加模型复杂度

[返回目录 ↑](#目录) | [下一章：模型评估与选择 →](#第2章-模型评估与选择)

## 第2章 模型评估与选择
### 2.1 经验误差与过拟合
#### 基本概念
1. **误差（Error）**
   - 训练误差（经验误差）：模型在训练集上的误差
   - 泛化误差：模型在新样本上的误差
   - 误差率：分类错误的样本数占总样本数的比例
   - 精度：1 - 误差率

2. **过拟合与欠拟合**
   - 过拟合（Overfitting）
     - 定义：模型在训练数据上表现很好，但泛化能力差
     - 原因：模型过于复杂，学习到了数据中的噪声
     - 解决方法：
       1. 增加训练样本
       2. 降低模型复杂度
       3. 正则化
       4. 早停

   - 欠拟合（Underfitting）
     - 定义：模型在训练数据上表现差
     - 原因：模型过于简单，无法捕捉数据特征
     - 解决方法：
       1. 增加模型复杂度
       2. 增加特征
       3. 减少正则化

#### 评估方法
1. **留出法（Hold-out）**
   - 原理：将数据集分为训练集和测试集
   - 特点：
     - 简单直观
     - 测试集应具有代表性
     - 多次随机划分取平均
   - 建议比例：
     - 训练集：2/3 ~ 4/5
     - 测试集：1/3 ~ 1/5

2. **交叉验证法（Cross Validation）**
   - k折交叉验证
     - 将数据集分成k个大小相似的互斥子集
     - 每次用k-1个子集训练，1个子集测试
     - 进行k次，取平均值
   - 留一法（LOO，Leave-One-Out）
     - k等于样本数量的特例
     - 优点：评估结果较为准确
     - 缺点：计算开销大

3. **自助法（Bootstrapping）**
   - 原理：有放回采样构建训练集
   - 特点：
     - 适用于数据集较小的情况
     - 允许在训练集中重复采样
     - 约36.8%的样本未被采样，用作测试集
   - 优缺点：
     - 优点：能够进行有效的评估
     - 缺点：改变了数据集的分布

### 2.2 性能度量
#### 回归任务性能度量
1. **均方误差（MSE）**
   - 定义：预测值与真实值差值的平方和的均值
   - 特点：对异常值敏感
   - 计算公式：MSE = (1/m)Σ(yi - ŷi)²

2. **平均绝对误差（MAE）**
   - 定义：预测值与真实值差值绝对值的均值
   - 特点：对异常值相对不敏感
   - 计算公式：MAE = (1/m)Σ|yi - ŷi|

#### 分类任务性能度量
1. **混淆矩阵（Confusion Matrix）**
   - 真正例（TP）：正类预测为正类
   - 假正例（FP）：负类预测为正类
   - 假负例（FN）：正类预测为负类
   - 真负例（TN）：负类预测为负类

2. **常用评估指标**
   - 准确率（Accuracy）
     - 定义：预测正确的样本数/总样本数
     - 公式：(TP + TN)/(TP + TN + FP + FN)
   
   - 精确率（Precision）
     - 定义：预测为正例的样本中真正例的比例
     - 公式：TP/(TP + FP)
   
   - 召回率（Recall）
     - 定义：真正例中被预测为正例的比例
     - 公式：TP/(TP + FN)
   
   - F1值
     - 定义：精确率和召回率的调和平均
     - 公式：2×P×R/(P + R)

3. **ROC与AUC**
   - ROC曲线
     - 横轴：假正例率（FPR）= FP/(FP + TN)
     - 纵轴：真正例率（TPR）= TP/(TP + FN)
     - 特点：反映了模型的分类性能
   
   - AUC（Area Under Curve）
     - 定义：ROC曲线下的面积
     - 范围：[0, 1]，越大越好
     - 特点：对样本类别分布不敏感

### 2.3 比较检验
#### 假设检验
1. **基本概念**
   - 显著度（α）：犯第一类错误的概率
   - p值：在原假设下观察到当前或更极端结果的概率
   - 置信度：1 - α

2. **常用检验方法**
   - t检验：比较两个算法的性能差异
   - 交叉验证t检验：基于交叉验证结果的显著性检验
   - McNemar检验：比较两个分类器的性能
   - Friedman检验：比较多个算法的性能

### 本章小结
1. 评估方法的选择要考虑数据规模、计算开销等因素
2. 性能度量需要根据任务类型选择合适的评估指标
3. 统计假设检验能够判断性能差异是否显著
4. 过拟合和欠拟合是机器学习中的两个基本问题
5. 模型选择要在偏差和方差之间找到平衡

### 习题与思考
1. 为什么要将数据集划分为训练集和测试集？
2. 交叉验证法相比留出法有什么优势？
3. 如何选择合适的性能度量指标？
4. ROC曲线为什么能够更好地评估分类器性能？
5. 如何处理数据集有类别不平衡的情况？

[返回目录 ↑](#目录) | [上一章：绪论 ←](#第1章-绪论) | [下一章：线性模型 →](#第3章-线性模型)

## 第3章 线性模型
### 3.1 基本形式
#### 线性回归
1. **一元线性回归**
   - 形式：f(x) = wx + b
   - 参数：
     - w：权重（斜率）
     - b：偏置（截距）
   - 优化目标：最小化均方误差
   - 求解方法：最小二乘法

2. **多元线性回归**
   - 形式：f(x) = w₁x₁ + w₂x₂ + ... + wᵢxᵢ + b
   - 向量形式：f(x) = w^T x + b
   - 矩阵形式：y = Xw + b
   - 求解方法：
     - 正规方程
     - 梯度下降
     - 随机梯度下降

#### 广义线性模型
1. **定义**
   - 将线性函数与非线性函数复合
   - g(y) = w^T x + b
   - g(·)为单调可微函数

2. **常见广义线性模型**
   - 对数线性回归
   - Logistic回归
   - Probit回归
   - Poisson回归

### 3.2 对数几率回归（Logistic回归）
#### 基本原理
1. **模型形式**
   - σ(z) = 1/(1 + e^(-z))
   - y = σ(w^T x + b)
   - 特点：输出值在[0,1]之间

2. **概率解释**
   - y表示正类的后验概率
   - 1-y表示负类的后验概率
   - ln(y/(1-y)) = w^T x + b

#### 参数估计
1. **极大似然估计**
   - 对数似然函数
   - 梯度上升/下降
   - 牛顿法
   - 拟牛顿法

2. **正则化**
   - L1正则化（Lasso）
   - L2正则化（Ridge）
   - 弹性网络

### 3.3 线性判别分析（LDA）
#### 基本思想
1. **投影思想**
   - 将数据投影到一条直线上
   - 使同类样本尽可能近
   - 使异类样本尽可能远

2. **数学表达**
   - 类内散度矩阵 Sw
   - 类间散度矩阵 Sb
   - 最大化 J = tr(Sw^(-1)Sb)

#### 算法步骤
1. **计算散度矩阵**
   - 计算类内散度矩阵
   - 计算类间散度矩阵
   - 计算总散度矩阵

2. **求解特征值问题**
   - 求解 Sw^(-1)Sb 的特征值和特征向量
   - 选择最大特征值对应的特征向量
   - 构造投影矩阵

### 3.4 多分类学习
#### 基本策略
1. **一对一（OvO）**
   - 构建 C(n,2) 个二分类器
   - 投票法确定最终类别
   - 优点：训练时间短
   - 缺点：测试时计算量大

2. **一对其余（OvR）**
   - 构建 N 个二分类器
   - 每个分类器将一个类别与其他类别区分
   - 优点：分类器数量少
   - 缺点：类别不平衡

3. **多对多（MvM）**
   - 每次将若干类作为正类
   - 其余类作为负类
   - 需要编码矩阵
   - 常用编码：
     - 纠错输出码（ECOC）
     - Hadamard码

### 3.5 类别不平衡问题
#### 解决方案
1. **数据层面**
   - 欠采样（Under-sampling）
   - 过采样（Over-sampling）
   - SMOTE算法
   - 集成采样

2. **算法层面**
   - 阈值移动
   - 代价敏感学习
   - 集成学习

### 本章小结
1. 线性模型是机器学习中最基础也是最重要的模型之一
2. 线性模型可以通过引入非线性变换得到强大的表达能力
3. 对数几率回归是处理分类问题的重要线性模型
4. LDA是一种经典的线性学习方法
5. 处理多分类问题时要注意类别不平衡问题

### 习题与思考
1. 为什么线性模型仍然被广泛使用？
2. Logistic回归与线性回归有什么区别和联系？
3. LDA与PCA的异同是什么？
4. 如何选择合适的多分类策略？
5. 在实际应用中如何处理类别不平衡问题？

### 补充知识
#### 优化算法
1. **梯度下降法**
   - 批量梯度下降（BGD）
   - 随机梯度下降（SGD）
   - 小批量梯度下降（Mini-batch GD）

2. **进阶优化算法**
   - 动量法（Momentum）
   - AdaGrad
   - RMSProp
   - Adam

#### 正则化技术
1. **L1正则化**
   - 特点：产生稀疏解
   - 应用：特征选择
   - 优势：模型简单，可解释性强

2. **L2正则化**
   - 特点：权重衰减
   - 应用：防止过拟合
   - 优势：计算简单，数值稳定

## 第4章 决策树
### 4.1 基本流程
#### 决策树的构成
1. **基本要素**
   - 节点：代表特征测试
   - 分支：代表测试结果
   - 叶节点：代表决策结果
   - 路径：从根节点到叶节点的路径代表决策规则

2. **决策树类型**
   - 分类树：预测离散类别
   - 回归树：预测连续值
   - 多变量决策树：每个节点可使用多个特征

#### 决策树生成
1. **基本算法框架**
   ```python
   def generate_tree(D, A):
       # D为数据集，A为特征集
       if 满足停止条件:
           return 创建叶节点
       选择最优特征a
       for 特征a的每个取值av:
           创建子节点
           递归构建子树
       return 决策树
   ```

2. **停止条件**
   - 当前节点样本全属于同一类别
   - 当前属性集为空或所有样本取值相同
   - 当前节点样本数小于阈值

### 4.2 特征选择
#### 信息增益
1. **信息熵**
   - 定义：H(D) = -Σp_i log_2(p_i)
   - 含义：度量数据集的纯度
   - 特点：值越小，纯度越高

2. **条件熵**
   - 定义：H(D|A) = Σ(|D_v|/|D|)H(D_v)
   - 含义：特征A对数据集D进行划分后的信息熵
   - 计算：对特征的每个取值计算信息熵并加权平均

3. **信息增益**
   - 定义：Gain(D,A) = H(D) - H(D|A)
   - 含义：特征A带来的信息纯度提升
   - 缺点：偏向取值较多的特征

#### 增益率
1. **定义**
   - Gain_ratio(D,A) = Gain(D,A)/IV(A)
   - IV(A) = -Σ(|D_v|/|D|)log_2(|D_v|/|D|)

2. **特点**
   - 对取值数目较多的特征有惩罚作用
   - 避免过度细分
   - C4.5算法采用此准则

#### 基尼指数
1. **定义**
   - Gini(D) = 1 - Σp_i²
   - 含义：从数据集中随机抽取两个样本类别不一致的概率

2. **基尼指数增益**
   - Gini_gain(D,A) = Gini(D) - Gini(D|A)
   - CART算法采用此准则
   - 计算速度快，效果好

### 4.3 剪枝处理
#### 预剪枝
1. **基本思想**
   - 在节点生成过程中就进行剪枝
   - 使用验证集评估是否继续划分

2. **优缺点**
   - 优点：
     - 防止过拟合
     - 减少训练时间
   - 缺点：
     - 可能导致欠拟合
     - 无法发现某些有价值的分支

#### 后剪枝
1. **基本思想**
   - 先生成完整决策树
   - 自底向上评估是否剪枝
   - 用验证集评估性能

2. **优缺点**
   - 优点：
     - 决策效果一般优于预剪枝
     - 比较可靠
   - 缺点：
     - 训练时间长
     - 需要额外的验证集

### 4.4 连续值与缺失值
#### 连续值处理
1. **二分法**
   - 确定候选划分点
   - 计算每个划分点的信息增益
   - 选择最优划分点

2. **多区间划分**
   - 聚类方法
   - 等频划分
   - 等宽划分

#### 缺失值处理
1. **训练集缺失值**
   - 样本权重：根据无缺失值样本比例调整
   - 特征值概率估计：根据其他样本分布估计

2. **预测时缺失值**
   - 主分支预测：选择样本数最多的分支
   - 多分支预测：将样本按比例分配到各分支

### 4.5 多变量决策树
#### 基本原理
1. **划分方式**
   - 线性组合划分
   - 非线性组合划分
   - 核方法

2. **优缺点**
   - 优点：
     - 更好的特征组合能力
     - 更简洁的树结构
   - 缺点：
     - 训练复杂度高
     - 可解释性降低

### 本章小结
1. 决策树是一种重要的机器学习方法，可解释性强
2. 特征选择是决策树学习的关键
3. 剪枝是避免过拟合的重要手段
4. 对连续值和缺失值的处理需要特殊方法
5. 多变量决策树能够处理更复杂的决策边界

### 习题与思考
1. 信息增益、增益率和基尼指数各有什么优缺点？
2. 为什么需要进行决策树剪枝？
3. 如何处理数值型特征？
4. 预剪枝和后剪枝的区别是什么？
5. 在实际应用中如何选择合适的决策树算法？

### 补充知识
#### 常见决策树算法
1. **ID3算法**
   - 使用信息增益选择特征
   - 只能处理离散特征
   - 容易过拟合

2. **C4.5算法**
   - 使用增益率选择特征
   - 能处理连续特征
   - 能处理缺失值
   - 有剪枝策略

3. **CART算法**
   - 使用基尼指数
   - 生成二叉树
   - 既可分类也可回归
   - 有完善的剪枝方法

#### 集成决策树
1. **随机森林**
   - 多棵决策树投票
   - 随机选择样本和特征
   - 降低方差

2. **梯度提升决策树**
   - 序列化生成树
   - 每棵树学习残差
   - 提高预测精度

## 第5章 神经网络
### 5.1 神经元模型
#### 生物神经元
1. **基本结构**
   - 树突：接收输入信号
   - 细胞体：处理信号
   - 轴突：传输输出信号
   - 突触：连接其他神经元

2. **信号传递**
   - 电化学信号传递
   - 兴奋或抑制
   - 累积效应
   - 阈值触发

#### M-P神经元模型
1. **数学表达**
   - y = f(Σw_ix_i + b)
   - f为激活函数
   - w_i为连接权重
   - b为偏置项

2. **激活函数**
   - 阶跃函数
   - Sigmoid函数
   - tanh函数
   - ReLU函数
   - Leaky ReLU

### 5.2 感知机与多层网络
#### 感知机
1. **结构特点**
   - 单层前馈网络
   - 二分类模型
   - 线性可分问题

2. **学习算法**
   - 梯度下降
   - 误差修正
   - 收敛性证明

#### 多层网络
1. **网络结构**
   - 输入层
   - 隐藏层
   - 输出层
   - 全连接结构

2. **前向传播**
   - 层间信息传递
   - 激活函数变换
   - 输出计算

### 5.3 误差逆传播算法
#### 基本原理
1. **误差计算**
   - 均方误差
   - 交叉熵
   - 输出层误差
   - 隐藏层误差

2. **参数更新**
   - 链式法则
   - 梯度下降
   - 学习率设置
   - 动量项

#### 算法流程
1. **前向传播阶段**
   ```python
   def forward(x):
       # 计算各层输出
       for layer in layers:
           x = layer.activate(x)
       return x
   ```

2. **反向传播阶段**
   ```python
   def backward(error):
       # 计算梯度并更新参数
       for layer in reversed(layers):
           error = layer.propagate_error(error)
           layer.update_params()
   ```

### 5.4 全局最小与局部极小
#### 局部极小问题
1. **产生原因**
   - 非凸优化
   - 参数空间复杂
   - 初始值敏感

2. **解决方法**
   - 多次随机初始化
   - 模拟退火
   - 随机梯度下降
   - 动量方法

#### 其他挑战
1. **梯度消失/爆炸**
   - 原因分析
   - 解决方案：
     - 合适的激活函数
     - 批量归一化
     - 残差连接

2. **过拟合**
   - 早停
   - 正则化
   - Dropout
   - 数据增强

### 5.5 其他常见神经网络
#### 卷积神经网络(CNN)
1. **基本组件**
   - 卷积层
   - 池化层
   - 全连接层
   - 特征图

2. **主要特点**
   - 局部连接
   - 权值共享
   - 空间下采样
   - 层次化特征

#### 循环神经网络(RNN)
1. **网络结构**
   - 循环连接
   - 状态保持
   - 时序处理
   - 变长序列

2. **变体**
   - LSTM
   - GRU
   - 双向RNN
   - 注意力机制

### 本章小结
1. 神经网络是一种强大的非线性学习方法
2. 误差逆传播是神经网络学习的核心算法
3. 深度学习中有多种网络结构和优化技术
4. 需要注意过拟合和局部极小等问题
5. 不同任务需要选择合适的网络结构

### 习题与思考
1. 为什么需要非线性激活函数？
2. 如何理解误差逆传播算法？
3. 局部极小问题如何解决？
4. CNN和RNN各自适用于什么任务？
5. 如何选择合适的网络结构和超参数？

### 补充知识
#### 深度学习框架
1. **PyTorch**
   - 动态计算图
   - Python优先
   - 研究友好
   - 生态丰富

2. **TensorFlow**
   - 静态计算图
   - 生产部署
   - 分布式训练
   - 跨平台支持

#### 高级优化技术
1. **自适应学习率**
   - AdaGrad
   - RMSprop
   - Adam
   - AdamW

2. **正则化技术**
   - L1/L2正则化
   - Dropout
   - 批量归一化
   - 层归一化

## 第6章 支持向量机
### 6.1 间隔与支持向量
#### 基本概念
1. **函数间隔**
   - 定义：y_i(w^T x_i + b)
   - 几何意义：点到超平面的距离
   - 标准化：||w|| = 1时等于几何间隔

2. **几何间隔**
   - 定义：y_i(w^T x_i + b)/||w||
   - 不随w,b等比例缩放而改变
   - 反映分类确信度

#### 最大间隔分类器
1. **优化目标**
   - 最大化：min(几何间隔)
   - 约束条件：y_i(w^T x_i + b) ≥ 1
   - 等价于最小化：1/2 ||w||²

2. **支持向量**
   - 距离超平面最近的样本点
   - 对应约束条件等号成立的点
   - 决定最终超平面位置

### 6.2 对偶问题
#### 拉格朗日对偶性
1. **原始问题**
   ```
   min 1/2 ||w||²
   s.t. y_i(w^T x_i + b) ≥ 1
   ```

2. **对偶问题**
   ```
   max Σα_i - 1/2 Σα_iα_jy_iy_j(x_i^T x_j)
   s.t. Σα_iy_i = 0, α_i ≥ 0
   ```

#### KKT条件
1. **互补松弛性**
   - α_i[y_i(w^T x_i + b) - 1] = 0
   - α_i = 0 或 y_i(w^T x_i + b) = 1

2. **求解方法**
   - SMO算法
   - 二次规划
   - 坐标下降

### 6.3 核函数
#### 核技巧
1. **基本思想**
   - 隐式定义特征空间
   - 避免高维计算
   - 核函数替代内积

2. **常用核函数**
   - 线性核：K(x,z) = x^T z
   - 多项式核：K(x,z) = (x^T z + 1)^d
   - 高斯核：K(x,z) = exp(-||x-z||²/2σ²)
   - sigmoid核：K(x,z) = tanh(βx^T z + θ)

#### 核函数选择
1. **选择原则**
   - 先验知识
   - 数据特点
   - 计算复杂度
   - 参数个数

2. **参数设置**
   - 网格搜索
   - 交叉验证
   - 经验法则

### 6.4 软间隔与正则化
#### 软间隔支持向量机
1. **引入松弛变量**
   - ξ_i ≥ 0
   - y_i(w^T x_i + b) ≥ 1 - ξ_i
   - 允许部分样本分类错误

2. **优化目标**
   ```
   min 1/2 ||w||² + C Σξ_i
   s.t. y_i(w^T x_i + b) ≥ 1 - ξ_i
        ξ_i ≥ 0
   ```

#### 正则化
1. **参数C的影响**
   - C越大，惩罚越大，趋向硬间隔
   - C越小，允许更多错误，防止过拟合
   - 需要通过交叉验证选择

2. **结构风险最小化**
   - 经验风险
   - 结构风险
   - 泛化性能

### 6.5 支持向量回归
#### SVR基本原理
1. **ε-不敏感损失**
   - |f(x) - y| ≤ ε 不计损失
   - 否则线性惩罚
   - 类似于软间隔

2. **优化问题**
   ```
   min 1/2 ||w||² + C Σ(ξ_i + ξ_i*)
   s.t. y_i - w^T x_i - b ≤ ε + ξ_i
        w^T x_i + b - y_i ≤ ε + ξ_i*
        ξ_i, ξ_i* ≥ 0
   ```

#### 求解方法
1. **对偶问题**
   - 引入拉格朗日乘子
   - 类似分类问题
   - 可使用核函数

2. **支持向量**
   - 落在ε管道上的点
   - 管道外的点
   - 决定回归函数

### 本章小结
1. SVM是一种有理论保证的优秀分类器
2. 核技巧使SVM能处理非线性问题
3. 软间隔增强了模型的鲁棒性
4. SVR扩展了SVM到回归问题
5. 优化算法是SVM的关键

### 习题与思考
1. 为什么要最大化间隔？
2. 核函数如何选择？
3. 软间隔与硬间隔的区别？
4. C参数如何影响模型性能？
5. SVM与其他分类器的比较？

### 补充知识
#### SVM优缺点
1. **优点**
   - 有理论保证
   - 可处理非线性
   - 泛化能力强
   - 避免维数灾难

2. **缺点**
   - 计算复杂度高
   - 核函数选择难
   - 对参数敏感
   - 解释性较差

#### 实践技巧
1. **数据预处理**
   - 特征缩放
   - 缺失值处理
   - 类别平衡
   - 降维

2. **模型选择**
   - 交叉验证
   - 网格搜索
   - 参数优化
   - 集成方法

## 第7章 贝叶斯分类器
### 7.1 贝叶斯决策论
#### 基本概念
1. **贝叶斯定理**
   - P(c|x) = P(x|c)P(c)/P(x)
   - 后验概率 = 似然度 × 先验概率 / 证据因子
   - MAP：最大后验概率估计
   - ML：最大似然估计

2. **判定准则**
   - 最小错误率贝叶斯决策
   - 最小风险贝叶斯决策
   - 拒绝选项

#### 贝叶斯最优分类器
1. **理论基础**
   - 条件风险最小化
   - 后验概率最大化
   - 贝叶斯最优性

2. **实际应用限制**
   - 类条件概率难以获得
   - 计算复杂度高
   - 需要大量数据

### 7.2 极大似然估计
#### 基本原理
1. **似然函数**
   - L(θ) = P(D|θ)
   - 对数似然
   - 独立同分布假设

2. **求解过程**
   ```python
   def MLE(data):
       # 构建似然函数
       # 取对数简化乘积
       # 求导得到极值点
       # 验证是否为极大值
   ```

#### 常见分布
1. **离散分布**
   - 伯努利分布
   - 二项分布
   - 多项分布
   - 泊松分布

2. **连续分布**
   - 正态分布
   - 指数分布
   - 伽马分布
   - t分布

### 7.3 朴素贝叶斯分类器
#### 基本假设
1. **条件独立性假设**
   - P(x|c) = ∏P(x_i|c)
   - 特征之间相互独立
   - 简化计算复杂度

2. **优缺点**
   - 优点：
     - 计算简单
     - 需要较少训练数据
     - 对缺失不敏感
   - 缺点：
     - 独立性假设过强
     - 零概率问题

#### 参数估计
1. **离散属性**
   - 频率估计
   - 拉普拉斯修正
   - 条件概率表

2. **连续属性**
   - 假设正态分布
   - 估计均值和方差
   - 核密度估计

### 7.4 半朴素贝叶斯分类器
#### 独立性放松
1. **独立性检验**
   - 卡方检验
   - 互信息
   - 条件互信息

2. **依赖结构**
   - SPODE
   - TAN
   - BAN

#### 参数学习
1. **结构学习**
   - 依赖关系图
   - 最大生成树
   - 贝叶斯网络

2. **参数估计**
   - 条件概率表
   - 期望最大化
   - 贝叶斯估计

### 7.5 贝叶斯网络
#### 网络结构
1. **有向无环图**
   - 节点表示随机变量
   - 边表示条件依赖
   - 局部马尔可夫性

2. **条件概率表**
   - 每个节点的CPT
   - 参数数量
   - 独立性编码

#### 推断与学习
1. **精确推断**
   - 变量消除
   - 信念传播
   - 结点树算法

2. **近似推断**
   - MCMC采样
   - 变分推断
   - 期望传播

### 本章小结
1. 贝叶斯方法是概率框架下的分类方法
2. 朴素贝叶斯是最简单实用的贝叶斯分类器
3. 半朴素贝叶斯通过放松独立性假设提高性能
4. 贝叶斯网络能表达复杂的依赖关系
5. 参数估计和结构学习是关键问题

### 习题与思考
1. 为什么需要朴素贝叶斯假设？
2. 如何处理连续特征？
3. 零概率问题如何解决？
4. 贝叶斯网络的优势是什么？
5. 如何选择合适的贝叶斯模型？

### 补充知识
#### 贝叶斯方法扩展
1. **变分贝叶斯**
   - 后验分布近似
   - 变分推断
   - 模型选择

2. **非参数贝叶斯**
   - 狄利克雷过程
   - 高斯过程
   - 印度自助餐过程

#### 实践应用
1. **文本分类**
   - 垃圾邮件过滤
   - 情感分析
   - 主题分类

2. **异常检测**
   - 欺诈检测
   - 故障诊断
   - 入侵检测

## 第8章 集成学习
### 8.1 个体与集成
#### 基本概念
1. **集成学习定义**
   - 多个学习器组合
   - 集成决策
   - 优于单一学习器
   - 分类/回归任务

2. **理论基础**
   - 误差-分歧分解
   - 偏差-方差分解
   - 多样性度量
   - No Free Lunch定理

#### 构建方法
1. **同质集成**
   - 同种类型基学习器
   - 如多个决策树
   - 随机森林

2. **异质集成**
   - 不同类型基学习器
   - 优势互补
   - Stacking方法

### 8.2 Boosting
#### AdaBoost算法
1. **基本原理**
   - 序列化生成
   - 样本重加权
   - 弱学习器组合
   - 前向分步算法

2. **算法流程**
   ```python
   def AdaBoost(D, T):
       # 初始化样本权重
       # for t in range(T):
       #     训练基学习器
       #     计算错误率
       #     更新样本权重
       #     更新学习器权重
       # return 集成学习器
   ```

#### 理论分析
1. **误差上界**
   - 指数损失函数
   - 训练误差界
   - 泛化误差界

2. **优缺点**
   - 优点：
     - 简单有效
     - 自适应调整
     - 无需调参
   - 缺点：
     - 对噪声敏感
     - 计算开销大
     - 难并行化

### 8.3 Bagging与随机森林
#### Bagging
1. **基本原理**
   - 自助采样
   - 并行学习
   - 投票/平均

2. **特点**
   - 降低方差
   - 防止过拟合
   - 易并行化
   - 对异常值稳健

#### 随机森林
1. **构建过程**
   - 随机样本采样
   - 随机特征选择
   - 决策树构建
   - 多数投票

2. **重要改进**
   - 特征重要性评估
   - OOB估计
   - 近邻分析
   - 缺失值处理

### 8.4 结合策略
#### 平均法
1. **简单平均**
   - 算术平均
   - 几何平均
   - 加权平均

2. **加权平均**
   - 固定权重
   - 动态权重
   - 性能权重

#### 投票法
1. **硬投票**
   - 多数投票
   - 绝对多数投票
   - 加权投票

2. **软投票**
   - 概率平均
   - 对数概率平均
   - 排序平均

#### 学习法
1. **Stacking**
   - 多层结构
   - 次级学习器
   - 交叉验证

2. **Blending**
   - 简化版Stacking
   - 单验证集
   - 计算效率高

### 8.5 多样性
#### 多样性度量
1. **分类任务**
   - 不合度量
   - κ统计量
   - Q-统计量

2. **回归任务**
   - 相关系数
   - 方差分解
   - 互补性

#### 多样性增强
1. **数据样本扰动**
   - 自助采样
   - 样本加权
   - 输入扰动

2. **输入特征扰动**
   - 随机子空间
   - 特征选择
   - 随机投影

### 本章小结
1. 集成学习通过组合多个学习器提高性能
2. Boosting和Bagging是两种主要的集成策略
3. 随机森林是最成功的集成学习方法之一
4. 结合策略对集成效果至关重要
5. 多样性是集成学习的核心

### 习题与思考
1. Boosting和Bagging的主要区别是什么？
2. 为什么随机森林效果好？
3. 如何选择合适的集成策略？
4. 多样性为什么重要？
5. 集成学习的局限性是什么？

### 补充知识
#### 高级集成方法
1. **XGBoost**
   - 正则化目标
   - 二阶近似
   - 列采样
   - 缺失值处理

2. **LightGBM**
   - GOSS采样
   - EFB特征打包
   - 直方图算法
   - 叶子优先生长

#### 实践技巧
1. **模型选择**
   - 基学习器选择
   - 集成大小确定
   - 参数优化
   - 计算资源平衡

2. **性能优化**
   - 特征工程
   - 参数调优
   - 模型压缩
   - 并行计算

## 第9章 聚类
### 9.1 聚类任务
#### 基本概念
1. **聚类定义**
   - 无监督学习
   - 相似对象分组
   - 组内相似性高
   - 组间相似性低

2. **应用场景**
   - 客户分群
   - 图像分割
   - 社交网络分析
   - 异常检测

#### 性能度量
1. **外部指标**
   - Rand指数
   - Jaccard系数
   - FM指数
   - 互信息

2. **内部指标**
   - DB指数
   - Dunn指数
   - 轮廓系数
   - 伪F统计量

### 9.2 距离计算
#### 距离度量
1. **闵可夫斯基距离**
   - 欧氏距离(p=2)
   - 曼哈顿距离(p=1)
   - 切比雪夫距离(p=∞)
   - 闵氏距离公式：
     d = (Σ|x_i - y_i|^p)^(1/p)

2. **其他距离**
   - 马氏距离
   - 余弦相似度
   - 汉明距离
   - 编辑距离

#### 相似度计算
1. **数值属性**
   - 标准化处理
   - 归一化处理
   - 缺失值处理

2. **非数值属性**
   - 名义属性
   - 有序属性
   - 混合属性
   - VDM度量

### 9.3 原型聚类
#### K均值算法
1. **基本流程**
   ```python
   def kmeans(X, k):
       # 随机选择k个初始中心
       # repeat:
       #     分配样本到最近中心
       #     更新聚类中心
       # until 收敛
       # return 聚类结果
   ```

2. **算法特点**
   - 简单高效
   - 对初始值敏感
   - 需预设簇数
   - 仅适用凸形状

#### 学习向量量化(LVQ)
1. **基本原理**
   - 竞争学习
   - 原型向量
   - 最近邻分配
   - 原型更新

2. **改进版本**
   - LVQ2.1
   - LVQ3
   - GLVQ
   - RSLVQ

### 9.4 密度聚类
#### DBSCAN算法
1. **核心思想**
   - 密度可达
   - 密度相连
   - 噪声点识别
   - 任意形状簇

2. **算法步骤**
   - 确定邻域参数
   - 识别核心点
   - 扩展簇
   - 处理噪声

#### 改进方法
1. **OPTICS**
   - 有序点集
   - 可达距离
   - 核心距离
   - 可视化辅助

2. **DENCLUE**
   - 密度函数
   - 密度吸引子
   - 高斯核函数
   - 快速聚类

### 9.5 层次聚类
#### 凝聚层次聚类
1. **基本策略**
   - 自底向上
   - 合并最近簇
   - 更新距离
   - 树形结构

2. **簇间距离**
   - 最小距离
   - 最大距离
   - 平均距离
   - Ward距离

#### 分裂层次聚类
1. **基本策略**
   - 自顶向下
   - 分裂选择
   - 二分聚类
   - 终止条件

2. **算法特点**
   - 计算开销大
   - 层次结构清晰
   - 无需预设簇数
   - 不可逆操作

### 本章小结
1. 聚类是重要的无监督学习方法
2. 距离度量是聚类的关键
3. 不同聚类算法各有特点
4. 聚类结果需要合理评估
5. 实际应用需要综合考虑

### 习题与思考
1. 如何选择合适的距离度量？
2. K均值算法的局限性是什么？
3. 密度聚类和层次聚类的区别？
4. 如何确定最佳簇数？
5. 如何处理大规模数据聚类？

### 补充知识
#### 高级聚类方法
1. **谱聚类**
   - 图切割
   - 拉普拉斯矩阵
   - 特征分解
   - 降维聚类

2. **模型聚类**
   - 高斯混合模型
   - EM算法
   - 贝叶斯聚类
   - 变分推断

#### 实践应用
1. **预处理**
   - 数据清洗
   - 特征选择
   - 降维处理
   - 异常检测

2. **结果分析**
   - 可视化
   - 有效性验证
   - 稳定性分析
   - 业务解释

## 第10章 降维与度量学习
### 10.1 k近邻学习
#### 基本概念
1. **k近邻算法**
   - 懒惰学习
   - 实例化学习
   - 非参数学习
   - 局部学习

2. **距离度量**
   - 欧氏距离
   - 曼哈顿距离
   - 闵可夫斯基距离
   - 马氏距离

#### 改进策略
1. **搜索优化**
   - kd树
   - 球树
   - 近似搜索
   - 局部敏感哈希

2. **距离计算**
   - 特征加权
   - 特征选择
   - 核技巧
   - 度量学习

### 10.2 降维
#### 维数灾难
1. **问题描述**
   - 数据稀疏
   - 距离计算失效
   - 计算复杂度高
   - 过拟合风险大

2. **解决方案**
   - 降维
   - 特征选择
   - 正则化
   - 核方法

#### 主成分分析
1. **基本原理**
   - 最大方差
   - 协方差矩阵
   - 特征值分解
   - 重构误差最小

2. **算法步骤**
   - 中心化
   - 计算协方差
   - 特征值分解
   - 投影变换

#### 核化线性降维
1. **核主成分分析**
   - 核技巧
   - 预映射
   - 核矩阵
   - 中心化

2. **核Fisher判别分析**
   - 类间散度
   - 类内散度
   - 广义特征值
   - 投影方向

### 10.3 度量学习
#### 问题描述
1. **基本概念**
   - 相似度学习
   - 距离度量
   - 马氏距离
   - 半正定矩阵

2. **学习目标**
   - 近邻关系
   - 相对距离
   - 绝对距离
   - 局部结构

#### 典型算法
1. **有监督度量学习**
   - 近邻成分分析
   - 相关成分分析
   - 大间隔最近邻
   - 信息理论度量学习

2. **半监督度量学习**
   - 约束传播
   - 流形正则化
   - 局部保持投影
   - 判别局部保持投影

### 本章小结
1. k近邻是重要的非参数学习方法
2. 降维可以缓解维数灾难
3. 主成分分析是最常用的降维方法
4. 度量学习可以自适应距离计算
5. 核方法可以处理非线性问题

### 习题与思考
1. k近邻算法的优缺点是什么？
2. 如何选择合适的降维方法？
3. PCA和LDA的区别是什么？
4. 度量学习的应用场景有哪些？
5. 如何处理高维数据的特征选择？

### 补充知识
#### 高级降维方法
1. **流形学习**
   - 等距映射
   - 局部线性嵌入
   - 拉普拉斯特征映射
   - t-SNE

2. **稀疏编码**
   - 字典学习
   - 压缩感知
   - 自编码器
   - 稀疏表示

#### 实践应用
1. **数据预处理**
   - 归一化
   - 标准化
   - 缺失值处理
   - 异常检测

2. **可视化技术**
   - 散点图矩阵
   - 平行坐标图
   - 雷达图
   - 热力图

[返回目录 ↑](#目录)

## 附录
### 数学基础
#### 线性代数
1. **矩阵运算**
   - 矩阵乘法
   - 矩阵分解
   - 特征值和特征向量
   - 奇异值分解

2. **向量空间**
   - 线性相关性
   - 基和维数
   - 正交和投影
   - 内积空间

#### 概率论
1. **基本概念**
   - 概率公理
   - 条件概率
   - 贝叶斯定理
   - 随机变量

2. **概率分布**
   - 离散分布
   - 连续分布
   - 期望与方差
   - 大数定律与中心极限定理

### 最优化基础
#### 无约束优化
1. **梯度下降**
   - 一阶导数
   - 学习率
   - 收敛性
   - 局部最优

2. **进阶方法**
   - 牛顿法
   - 拟牛顿法
   - 共轭梯度法
   - 随机梯度下降

#### 约束优化
1. **等式约束**
   - 拉格朗日乘子法
   - KKT条件
   - 对偶问题
   - 互补松弛性

2. **不等式约束**
   - 可行域
   - 内点法
   - 障碍函数法
   - 惩罚函数法

[返回目录 ↑](#目录)
